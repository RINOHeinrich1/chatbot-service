
---

## ğŸ§  1. Notions de base en NLP (Traitement du langage naturel)

### âœ… **Tokenization**

* Transformer une phrase en **tokens** (morceaux de mots ou mots).
* Ex: `"Paris est belle"` â†’ `["Paris", "est", "belle"]` â†’ puis en IDs numÃ©riques via le tokenizer.

### âœ… **Embeddings**

* ReprÃ©sentation numÃ©rique dense dâ€™un texte, permettant de **comparer sÃ©mantiquement** deux textes.
* Exemple : `"Paris est en France"` â‰ˆ `"La capitale de la France est Paris"` grÃ¢ce Ã  des embeddings proches.

### âœ… **ModÃ¨les prÃ©-entraÃ®nÃ©s**

* `sentence-transformers/all-MiniLM-L6-v2` : gÃ©nÃ¨re des embeddings.
* `moussaKam/barthez` : gÃ©nÃ¨re du texte en franÃ§ais.

---

## ğŸ” 2. Recherche sÃ©mantique avec FAISS

### âœ… **FAISS (Facebook AI Similarity Search)**

* Librairie rapide pour faire de la recherche dans de grands ensembles vectoriels.
* Utile pour retrouver les documents **les plus proches** dâ€™une requÃªte.

### âœ… **Index FAISS**

* Structure de donnÃ©es optimisÃ©e pour retrouver rapidement les `k` documents les plus similaires.

### âœ… **Distance L2 / Cosine**

* Le code utilise `IndexFlatL2` = **distance euclidienne**.
* Plus la distance est petite, plus les phrases sont proches sÃ©mantiquement.

---

## âš™ï¸ 3. GÃ©nÃ©ration de rÃ©ponse (RAG)

### âœ… **RAG (Retrieval-Augmented Generation)**

* Pipeline **classique** :
  â†’ RequÃªte utilisateur â†’ Recherche de documents â†’ GÃ©nÃ©ration de rÃ©ponse Ã  partir des documents.
* Tu guides le modÃ¨le de gÃ©nÃ©ration **uniquement avec le contexte extrait**.

### âœ… **Prompt engineering**

* Structure du prompt pour forcer le modÃ¨le Ã  :

  * Ne **rÃ©pondre quâ€™avec** le contexte.
  * Dire **"Je ne sais pas"** si ce nâ€™est pas dans le contexte.

---

## ğŸ§ª 4. Optimisation et industrialisation

### âœ… **GPU / CPU**

* `torch.cuda.is_available()` permet d'utiliser un GPU si dispo, ce qui accÃ©lÃ¨re l'infÃ©rence.

### âœ… **Cache d'embeddings**

* Pour Ã©viter de recalculer les embeddings et lâ€™index Ã  chaque lancement â†’ ils sont **sauvegardÃ©s sur disque**.

### âœ… **Cache de rÃ©ponses**

* Tu utilises un `dict` Python (`cache`) pour Ã©viter de gÃ©nÃ©rer deux fois la mÃªme rÃ©ponse.

---

## ğŸ“š 5. Python & BibliothÃ¨ques

### ğŸ“¦ `transformers` (Hugging Face)

* Pour le **tokenizer**, les **modÃ¨les de gÃ©nÃ©ration**, et les **pipelines** NLP.

### ğŸ“¦ `datasets`

* Permet de manipuler des bases de donnÃ©es textuelles (ici, `knowledge_base`) facilement.

### ğŸ“¦ `faiss`

* Pour indexer les embeddings et effectuer la recherche la plus rapide possible.

### ğŸ“¦ `torch` (PyTorch)

* Pour manipuler les tensors, faire de lâ€™infÃ©rence sur les modÃ¨les.

---

## ğŸ§© En rÃ©sumÃ© (connaissances clÃ©s)

| Domaine                  | Notions Ã  savoir                                        |
| ------------------------ | ------------------------------------------------------- |
| **NLP**                  | Tokenizer, embeddings, modÃ¨les prÃ©-entraÃ®nÃ©s            |
| **Vectorisation**        | Embeddings, distances, batch processing                 |
| **Recherche sÃ©mantique** | FAISS, Index, top-k, seuil de similaritÃ©                |
| **GÃ©nÃ©ration**           | Prompt engineering, modÃ¨les seq2seq, pipeline text2text |
| **Optimisation**         | Cache, GPU vs CPU, sauvegarde de lâ€™index                |
| **Python/Librairies**    | `transformers`, `datasets`, `faiss`, `torch`            |

---
