import requests
import json
import os
from datetime import datetime
from config import *
from utils.helpers import *
from .cache import *
from .postgres import *
# === Fonctions utilitaires ===
import joblib

# Chargement du modÃ¨le et du vectorizer
model = joblib.load("request_classifier_model.pkl")
vectorizer = joblib.load("request_vectorizer.pkl")

def is_question_or_request(text: str) -> bool:
    X = vectorizer.transform([text])
    prediction = model.predict(X)[0]
    return bool(prediction)

def call_llm(model, messages, temperature=0.5, max_tokens=300):
    payload = {
        "model": model,
        "messages": messages,
        "temperature": temperature,
        "max_tokens": max_tokens,
    }
    headers = {
        "Authorization": f"Bearer {AI_TOKEN}",
        "Content-Type": "application/json",
    }
    response = requests.post(AI_URL, headers=headers, data=json.dumps(payload))
    response.raise_for_status()
    return response.json()["choices"][0]["message"]["content"].strip()

def build_contexte(docs):
    return "\n---\n".join(
        f"{doc['text']}\n(Source: {doc.get('source', 'inconnu')})" for doc in docs
    )

def get_chatbot_description(chatbot_id):
    try:
        res = supabase.table("chatbots").select("description").eq("id", chatbot_id).single().execute()
        return res.data.get("description", "").strip()
    except Exception:
        return ""

def get_connexion_info(chatbot_id):
    try:
        res = supabase.table("chatbot_pgsql_connexions") \
            .select("connexion_name, sql_reasoning") \
            .eq("chatbot_id", chatbot_id) \
            .single().execute()
        name = res.data["connexion_name"]
        enabled = res.data.get("sql_reasoning", False)

        schema, params = "", {}
        if enabled:
            res_conn = supabase.table("postgresql_connexions") \
                .select("data_schema, host_name, port, user, password, database, ssl_mode") \
                .eq("connexion_name", name) \
                .single().execute()
            schema = res_conn.data.get("data_schema", "").strip()
            params = res_conn.data
        return name, enabled, schema, params
    except Exception as e:
        print(f"[Erreur chargement SQL reasoning ou schÃ©ma] : {e}")
        return "", False, "", {}

def build_system_prompt(query,description, sql_reasoning_enabled, schema_text,discu):
    prompt = (
        "Tu es un assistant intelligent, clair et naturel. "
        f"Tu suis la consigne suivante : {description or 'rÃ©ponds poliment et avec clartÃ©.'} "
    )
    if sql_reasoning_enabled and schema_text:
        prompt += (
            f"\n\nVoici les tables de la base de donnÃ©e postgres avec leurs colonnes respectif :\n{schema_text}\n\n"
            f"\nEn te basant sur ces informations sur la base de donnÃ©e, donne  un requÃªte SQL pour rÃ©pondre au question :\"{query}\", en respectant les rÃ¨gles:\n"
            "1. Retourne uniquement une requÃªte SQL PostgreSQL valide, exÃ©cutable.\n"
            "2. Ne retourne jamais  d'explication ou de commentaire, de balises Markdown ou des Ã©chappements via \. Juste la requÃªte\n"
            "3. La requÃªte doit toujours Ãªtres complÃ¨tes"
            "4. Les noms de colonnes et de tables dans la requÃªte doivent obligatoirement Ãªtre mis entre guillemets anglais.\n"
            "5. Indique toujours la table utilisÃ©e.\n"
            "6. La requÃªte doit Ãªtre sur une seule ligne.\n"
        )
      #  print(f"Prompt: {prompt}")

    else:
        prompt += "\n\nSi tu ne trouves pas la rÃ©ponse dans les contextes fournis, indique que l'information n'est pas disponible."
    return prompt

# === Fonctions principales ===

def reformulate_answer_via_llm(query, contexte_text):
    messages = [
        {"role": "system", "content": "Tu es un assistant intelligent, clair et naturel."},
        {"role": "user", "content": (
            "Tu es un assistant qui reformule une rÃ©ponse claire, naturelle et concise pour un utilisateur.\n"
            f"Voici la question posÃ©e :\n{query}\n\n"
            f"Voici le contexte complet :\n{contexte_text}\n\n"
            "RÃ©pond uniquement au question posÃ©e en franÃ§ais, ne donne pas de dÃ©tails technique ni de la faÃ§on dont tu as obtenue la rÃ©ponse\n"
        )}
    ]
    return call_llm("mixtral", messages)

def ask_mixtral_for_relevant_sources(chatbot_id: str, question: str):
    if not is_question_or_request(question):
        return [{"type": "aucun", "name": "aucun"}]

    sources = []

    for c in get_connexions_for_chatbot(chatbot_id):
        sources.append({
            "type": "connexion",
            "name": c["connexion_name"],
            "description": c["description"]
        })

    for d in get_documents_for_chatbot(chatbot_id):
        sources.append({
            "type": "document",
            "name": d["document_name"],
            "description": d["description"]
        })

    if not sources:
        return [{"type": "aucun", "name": "aucun"}]

    print(f"Liste des sources:{sources}")

    prompt = (
        "Tu es un assistant intelligent chargÃ© de sÃ©lectionner les sources les plus pertinentes pour rÃ©pondre Ã  une question.\n"
        f"Question :\n{question}\n\n"
        "Sources disponibles :\n"
        f"{json.dumps(sources, ensure_ascii=False)}\n\n"
        "RÃ©ponds uniquement par la liste JSON EXACTE au format :\n"
        "[{\"type\": \"document\" ou \"connexion\", \"name\": \"nom_de_la_source\"}, ...]\n"
        "Si la question n'est pas une demande (exemple: salutation, remerciement ou autres), tu ne renvoies :\n"
        "[{\"type\": \"aucun\", \"name\": \"aucun\"}]\n"
        "La liste doit contenir uniquement les sources sÃ©lectionnÃ©es, sans aucune explication, commentaire, texte supplÃ©mentaire ou guillemets inversÃ©s.\n"
        "Si aucune source n'est pertinente, retourne une liste vide : []\n"
        "Ne rÃ©ponds jamais autre chose que cette liste JSON."
    )

    messages = [
        {"role": "system", "content": "Tu es un assistant intelligent qui sÃ©lectionne les sources pertinentes."},
        {"role": "user", "content": prompt}
    ]

    result = call_llm("mixtral", messages)

    try:
        return json.loads(result)
    except Exception:
        return result
import re

def corriger_sql_heuristique(sql):
    # Mots-clÃ©s SQL Ã  ne jamais mettre entre guillemets
    mots_cles = {
        "SELECT", "FROM", "JOIN", "INNER", "LEFT", "RIGHT", "FULL", "OUTER", "ON",
        "WHERE", "AS", "AND", "OR", "GROUP", "BY", "ORDER", "HAVING", "DESC", "ASC",
        "LIMIT", "OFFSET", "IN", "IS", "NULL", "NOT", "UNION"
    }

    # Fonction pour mettre des guillemets sur identifiants si pas dÃ©jÃ  faits
    def quote_identifier(ident):
        ident = ident.strip()
        if ident.upper() in mots_cles or re.match(r'^["\'].*["\']$', ident) or ident.isdigit():
            return ident
        return f'"{ident}"'

    # Corriger les a.b â†’ "a"."b"
    def corriger_dot_expr(match):
        left, right = match.group(1), match.group(2)
        return f'{quote_identifier(left)}.{quote_identifier(right)}'

    # Corriger AS alias
    def corriger_alias(match):
        base, alias = match.group(1), match.group(2)
        return f'{base} AS {quote_identifier(alias)}'

    # Ã‰tape 1 : corriger a.b
    sql = re.sub(r'(\w+)\.(\w+)', corriger_dot_expr, sql)

    # Ã‰tape 2 : corriger alias sans guillemets
    sql = re.sub(r'AS\s+(\w+)', lambda m: f'AS {quote_identifier(m.group(1))}', sql, flags=re.IGNORECASE)

    # Ã‰tape 3 : corriger noms de table/colonne simples dans SELECT ou FROM/JOIN
    def corriger_mots_simples(match):
        mot = match.group(0)
        return quote_identifier(mot)

    # SÃ©parer les blocs SELECT ... FROM ...
    pattern_select_from = re.search(r'SELECT\s+(.*?)\s+FROM\s+', sql, flags=re.IGNORECASE | re.DOTALL)
    if pattern_select_from:
        select_block = pattern_select_from.group(1)
        corrected = []

        for part in select_block.split(','):
            part = part.strip()
            if "AS" not in part.upper() and "." not in part:
                part = quote_identifier(part)
            corrected.append(part)
        sql = sql.replace(select_block, ', '.join(corrected))

    # Corriger noms simples aprÃ¨s FROM ou JOIN
    sql = re.sub(r'\b(FROM|JOIN)\s+(\w+)', lambda m: f'{m.group(1)} {quote_identifier(m.group(2))}', sql, flags=re.IGNORECASE)

    return sql

def extract_sources(docs):
    return list({doc.get("source", "inconnu") for doc in docs})

def generate_answer(query, docs, chatbot_id=None, max_retries=3):
    cached = get_cache(query, docs)
    if cached:
        return {
            "answer": cached,
            "sql": None,
            "sources": extract_sources(docs),
        }

    description = get_chatbot_description(chatbot_id)
    connexion_name, sql_reasoning_enabled, schema_text, connexion_params = get_connexion_info(chatbot_id)

    system_prompt = build_system_prompt(query, description, (sql_reasoning_enabled and len(docs)>0), schema_text, "")
    contexte = build_contexte(docs)

    sources_used = extract_sources(docs)
    extracted_sql = None

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": (
            f"Voici le contexte :\n{contexte.strip()}\n\n"
            f"Voici la question :\n{query.strip()}"
        )}
    ]

    try:
        raw_result = call_llm("mixtral", messages, temperature=0, max_tokens=300)
        print("ðŸ”§ Contenu brut du LLM:\n", raw_result)
    except Exception as e:
        raw_result = f"Erreur lors de la gÃ©nÃ©ration de la rÃ©ponse : {str(e)}"
        set_cache(query, docs, raw_result)
        return {
            "answer": raw_result,
            "sql": None,
            "sources": sources_used,
        }

    final_answer = raw_result
    if sql_reasoning_enabled and len(docs)>0:
        retry_count = 0
        tried_heuristic = False  # Pour ne corriger heuristiquement qu'une fois

        while retry_count <= max_retries:
            extracted_sql = extract_sql_from_text(raw_result)
            if extracted_sql is None:
                print("Aucune requÃªte SQL extraite.")
                break

            if os.environ.get("DEBUG_SQL"):
                print(f"SQL Ã  exÃ©cuter (essai #{retry_count + 1}): {extracted_sql}")

            try:
                # 1Ã¨re tentative ou tentative aprÃ¨s LLM/heuristique
                sql_result = execute_sql_via_api(connexion_params, extracted_sql)
                if sql_result is not None:
                    print(f"RÃ©sultat SQL: {sql_result}")
                    docs.insert(0, {
                        "text": (
                            f"RÃ©sultat SQL pour la requÃªte suivante : {query}\n"
                            f"Code SQL : {extracted_sql}\n\n"
                            f"RÃ©sultat :\n{json.dumps(sql_result, indent=2, ensure_ascii=False)}"
                        ),
                        "source": "rÃ©sultat_sql"
                    })
                    final_answer = reformulate_answer_via_llm(query, build_contexte(docs))
                    break  # SuccÃ¨s
                else:
                    raise Exception("RÃ©sultat SQL vide ou invalide")

            except Exception as e:
                print(f"Erreur exÃ©cution SQL: {e}")

                # Tentative avec correcteur heuristique
                if not tried_heuristic:
                    print("â›‘ï¸ Tentative de correction heuristique...")
                    extracted_sql = corriger_sql_heuristique(extracted_sql)
                    tried_heuristic = True
                    continue  # RÃ©essayer avec la requÃªte corrigÃ©e
                 
                if retry_count == max_retries:
                    final_answer = f"Erreur lors de l'exÃ©cution de la requÃªte SQL : {e}\nRequÃªte SQL : {extracted_sql}"
                    break

                # Appel LLM comme dernier recours
                correction_prompt = [
                    {"role": "system", "content": "Tu es un assistant SQL expert qui corrige les requÃªtes SQL erronÃ©es."},
                    {"role": "user", "content": (
                        f"La requÃªte SQL suivante a provoquÃ© une erreur lors de son exÃ©cution :\n{extracted_sql}\n"
                        f"Erreur : {e}\n"
                        "Merci de corriger cette requÃªte SQL pour qu'elle soit valide et exÃ©cutable en PostgreSQL.\n"
                        "Retourne uniquement la requÃªte SQL corrigÃ©e, sans explications."
                    )}
                ]
                raw_result = call_llm("mixtral", correction_prompt, temperature=0, max_tokens=200)
                retry_count += 1
        else:
            # Si on sort de la boucle sans break (pas de requÃªte SQL correcte)
            final_answer = reformulate_answer_via_llm(query, contexte)

    set_cache(query, docs, final_answer)

    return {
        "answer": final_answer,
        "sql": extracted_sql,
        "sources": sources_used,
    }
